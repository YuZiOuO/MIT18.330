{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 18.330 Problem set 4 (spring 2020)\n\n## Submission deadline: 11:59pm on Tuesday, March 3\n\n### Exercise 1: Summation two ways\n\nConsider the problem of summing $N$ numbers:\n\n$$\nS = \\sum_{n=1}^{N} x_n\n$$\nWe will compare two different ways to do this with the built-in  `sum` function in Julia.\n\n1. Implement the naive summation algorithm as `sum1`\n    that takes in a vector `x` and sums the elements, using\n    e.g. a `for` loop.\n\n2. Implement the summation in a different way (`sum2`) by\n    splitting the vector into two halves\n    and calling the function recursively on each half.\n    The base case when `x` is of length 1 just returns `x[1]`.\n    For simplicity assume that `x` has a length that is a power\n    of two.\n\n    The recursive line of your implementation should look something like this:\n\n    `return sum2(view(x, 1:nnew)) + sum2(view(x, nnew+1:n))`\n\n    This is the recursive part. `nnew` is half the length of the input vector,\n    `n/2` (you might want to use `div` to calculate this).\n    While not specifically part of the course, the reason you need to use views\n    is to prevent excessive memory allocations that slow down the code too\n    much for this problem to work -- so please use them.\n\n3. Use the built-in Julia `sum` function on `big.(x)` as the true value.\n    Calculate the relative error of `sum1` and `sum2` as\n    the length of `x` is varied from $2^5$ to $2^{20}$.\n    Make a plot of relative error vs. length.\n    What do you see? Which plot function would you use only given this\n    plot?\n\n4. How do the speeds compare? Using the `@belapsed` function, eg.\n\n    ```jl\n    sum1_time = `@belapsed sum1($x)`\n    ```\n\n    benchmark the two sum functions that you have defined and plot the run times as a\n    function of length. Which function would you use only given this plot?\n\n5. If everything went according to plan you should have obtained\n    different answers for [3] and [4]. How can we remedy this\n    situation? Recursing all the way down to a base case of length $1$\n    is very inefficient, since there is a time overhead for each\n    function invocation. From your error plot, though, you should be\n    able to see that the errors of your implementations\n    only start diverging once `x` reaches\n    a certain length, say `2^10` (although you can play with this\n    number).\n\n    Write a new function `sum3` that uses recursion but that\n    calls `sum1` after `x` is sufficiently short, i.e. changing\n    the base case specification in your `sum2` function.\n\n    [This a technique that we will see repeatedly with\n    other recursive algorithms, such as cache-efficient\n    matrix multiplication and the FFT algorithm.]\n\n\n6. Calculate the relative error and run time for this\n    new function and also the built-in Julia `sum` function as\n    in [3, 4] and add it the plots you made there. Which function\n    would you use now? Your `sum3` implementation is the same way\n    that julia calculates sums bit with more optimizations.\n\nHopefully this problem has shown that thinking about errors\nand how you implement even the simplest algorithms,\nsuch as summation, is important if you want to have both\nperformance and accuracy. Naive implementations can be\ndangerous!\n\n\n#### Exercise 2: Conditioning of a problem and stability of an algorithm\n\nConsider the problem to calculate\n\n$$\\phi(x) = \\sqrt{1 + x} - 1$$\n\nclose to $x = 0$.\n\n1. Calculate the relative condition number $\\kappa_\\phi(x)$.\n\n2. Is the problem well conditioned near $x = 0$?\n\n3. Consider the obvious algorithm by breaking up the formula into its simplest\n    pieces. Thinking about it as this series of\n    algorithmic steps, calculate the condition number of each step for $x$ near $0$.\n    Which is the problematic step?\n\n4. By using an algebraic manipulation, find an alternative\n    algorithm to evaluate the function which is \"stable\", i.e. which does not\n    introduce extra numerical error in this way.\n\n5. Implement both the obvious algorithm and your better algorithm. Take the\n    true value of $\\phi$ to be `Ï•(big(x))` and calculate the relative error of\n    both algorithms for different values of $x$ as it decreases toward $0$.\n    Plot the errors as a function of $x$ on the same axes. Is your new algorithm\n    better?\n\n#### Exercise 3: Conditioning of polynomial roots\n\nConsider a degree-$n$ polynomial\n$p(x) = a_0 + a_1 x + \\cdots + a_n x^n$.\n\n1. Show that the condition number of the problem of finding a root $r$\n    of this polymomial when the leading coefficient $a_n$ is varied is\n\n    $$\\kappa = \\left| \\frac{a_n r^{n-1}}{p'(r)} \\right|$$\n\n    Hint: To do this use e.g. implicit differentiation on the equation that\n    the root $r$ satisfies.\n\n2. Consider the (in)famous Wilkinson polynomial\n\n    $p(x) = (x - 1) (x - 2) \\cdots (x - 20)$.\n\n    Calculate the coefficients of the polynomial using e.g.\n    your `Polynomial` type from a previous problem set or the\n    `Polynomials.jl` package. Note that you will need to use\n    `Int128` or `BigInt`.\n\n3. Calculate, e.g. analytically or using automatic differentiation,\n    the condition number of each root of the polynomial. Which roots are\n    well-conditioned and which are ill-conditioned?\n\n4. Use e.g. the `roots` function from the `PolynomialRoots.jl` package\n    to calculate the roots of $p(x)$. Are they correct?\n\n    Now perturb a single coefficient of $p(x)$ by `randn()*2.0^(-23)`` and find\n    and plot the roots on the same graph. Repeat this 50 times to see\n    a visual representation of the unstable nature of polynomial root finding.\n\n    Does the result agree with what you calculated in [3]?\n\n\n#### Exercise 4: Lagrange interpolation\n\n1. Given nodes $t_i$, write a function to\n    calculate the Lagrange cardinal functions $\\ell_k(x)$ that satisfies\n    $\\ell_k(t_i) = [i = k]$ (remember $[i=k]$ is 0 unless $i=k$).\n    Your function should take in a vector `t`, a value `x` and the index `k`\n    and return the value of $\\ell_k(x)$. What is the operation count for\n    calculating the cardinal function?\n\n2. Write a function to calculate the Lagrange interpolant in\n    nodes $t_i$ with data $y_i$ at the point $x$ using your lagrange cardinal\n    function from [1]. What is the operation count for calculating the Lagrange\n    interpolant?\n\n3. Find the Lagrange interpolant to the function $\\exp(x)$ sampled\n    at $N + 1$ equally spaced points in the interval $[-1, 1]$. Make a plot\n    of the interpolant over this interval when $N = 10$. Also add a plot of\n    $\\exp(x)$ using the built in function and a scatter plot of the interpolating\n    points. Do you see what you expect.\n\n4. Calculate numerically the error in the interpolant compared to the function\n    $\\exp(x)$ by sampling at, say, 100 points\n    in the interval and taking the maximum error.\n    How does the error vary with the degree of the interpolant?\n\n5. Consider the function $f(x) = \\frac{1}{1 + 25x^2}$ on the interval $[-1, 1]$.\n    What happens as you increase the number of equally-spaced points?\n    Make an interactive visualization.\n    This is known as the **Runge phenomenon**\n\n6. Instead, interpolate at the **Chebyshev points**,\n    $t_i := \\cos(\\theta_i)$, where $\\theta_i$\n    are equally-spaced angles between $0$ and $\\pi$. Make another interactive visualization.\n    You should find that now we do have convergence.\n    Find numerically how the rate of convergence of the distance\n     between the interpolant and the true function behaves as the number of points increases.\n    This is known as **spectral convergence**."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.3.1"
    },
    "kernelspec": {
      "name": "julia-1.3",
      "display_name": "Julia 1.3.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
